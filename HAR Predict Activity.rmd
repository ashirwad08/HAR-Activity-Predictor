---
title: "HAR Predict Activity"
author: "Ash Chakraborty"
date: "December 21, 2015"
output: html_document
---  

```{r global, ECHO=FALSE, message=FALSE, warning=FALSE, error=FALSE}
library(caret)
library(GGally)
train.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```  

# OVERVIEW  

"Human Activity Recognition" (HAR) exercise. Use data collected from a wearable
device to train a couple of machine learning models so that they can "learn" to
predict the "type" of activity ('classe') given a test data set.  

The link and the project are here: http://groupware.les.inf.puc-rio.br/har

# DATA PREVIEW  

Training data is at: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

In order to get an idea of the _out of sample_ error we'll need a seperate, untouched cross validation set.  

```{r read_testtrain}
raw <- read.csv(train.url, header = T, na.strings = c(""," ","#DIV/0!","NA"))

set.seed(31412)
trainInds <- createDataPartition(raw$classe, p=.7, list = F)
#names, ids aren't likely predictors
#timestamps aren't in a good series, removing
train <- raw[trainInds, ] 
cv <- raw[-trainInds, ]
```  

## Exploratory Feature Analysis  

The dataset has 159 potential predictors for the outcome variable "classe". We now explore these potentials to evaluate their suitability for inclusion in the classification exercise. We'll achieve this by evaluating the following:  

### Non-Relevant Variables  
We see that the first 5 columns consist of subject ids, names, and activity timestamps. We eliminate these because they aren't likely contributors to the statistical model.  
```{r eda_nonrelevance}
train <- train[,-c(1:5)]
cv <- cv[,-c(1:5)]
```  

### Unknown Values:  
Since we want to train our model on as complete a dataset as possible, we conduct an exercise to only keep features where at least 70% of the values are _known_.  
```{r eda_unknowns}
# Impute NAs; NZV has also removed some NAs. I want to remove variables where
# more than 70% of the values are unknown.
train <- train[, -which(colSums(is.na(train))/nrow(train) > 0.70)]

```   

#### Imputing Unknowns  
For the remaining unknowns we choose an "knnImpute" method to estimate the missing values. This helps preserve valuable features that might otherwise have been lost to the analysis on account of missing data.  

### Lack of Variance  
Features, especially discrete types, that exhibit near zero variability will not inform the analysis. It's therefore prudent to remove these features from the analysis. In order to do so, we make use of the caret package's _nearZeroVar()_ method. It disqualifies features based on the following criteria, as listed in its vignette:  
* they (potential predictors) have very few unique values relative to the number of samples,  
* the ratio of the frequency of the most common value to the frequency of the second most common value is large.  

```{r eda_nzv}
train <- train[,-nearZeroVar(train, saveMetrics = F)]
```  

### Multicollinearity of Features  
Finally, although the plan is to use _classification tree_ based algorithms for the analysis, and these typically do well with co-dependent features, we want to remove features that exhibit a very high degree of correlation between each other. This will help in eliminating additional superfluous features and optimize the training time.

```{r multicollinearity}
train.cor <- cor(train[,-54])
high.train.cor <- findCorrelation(train.cor, 0.95)
#visualize correlations
ggpairs(data=train, columns=high.train.cor,54)
train <- train[,-high.train.cor]
```   

# BUILDING AND TUNING MODELS  

## Model 1: Bagged FDA  

```{r train}
library(doMC)
registerDoMC(cores=8)
# Might want to center and scale, want to try different algorithms
system.time(
bagFDA.fit <- train(classe~., data=train, preProcess=c('center','scale'), 
                   method="bagFDA")
)

```  
## Model 2: Random Forest  

## Model 3: Support Vector Machine  

```{r svmTrain}
library(doMC)
registerDoMC(cores=8)
system.time(
        svm.fit <- train(classe~., data=train,
                         preProcess=c('center','scale'),
                         method='svmRadial',
                         tuneLength=5)
        
)
```


# PREDICTING AND CROSS VALIDATING RESULTS 

We first predict results on our untouched cross validation set by each model trained. Then we evaluate the prediction accuracy by using a confusion metric and ROC curves.

```{r predcv}
models <- list(bagFDA.fit, svm.fit)
pred <- predict(models, newdata=cv)
predValues <- extractPrediction(models, testX = cv, testY = cv$classe)
testValues <- subset(predValues, dataType=='Test')
probValues <- extractProb(models, testX = cv, testY = cv$classe)
testProbs <- subset(probValues, dataType=='Test')
plotClassProbs(testProbs)
```






