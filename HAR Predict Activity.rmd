---
title: "HAR Predict Activity"
author: "Ash Chakraborty"
date: "December 21, 2015"
output: html_document
---  

```{r global, ECHO=FALSE, message=FALSE, warning=FALSE, error=FALSE}
library(caret)
library(ggplot2)
library(GGally)
library(foreach)
library(doParallel)
train.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```  

# OVERVIEW  

"Human Activity Recognition" (HAR) exercise. Use data collected from a wearable
device to train a couple of machine learning models so that they can "learn" to
predict the "type" of activity ('classe') given a test data set.  

The link and the project are here: http://groupware.les.inf.puc-rio.br/har

# DATA PREVIEW  

Training data is at: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

We'll need a cross validation set, since the testing set is provided separately, here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

So, let's read in data, and create a Cross Validation set:  

```{r testtrain}
raw <- read.csv(train.url, header = T, na.strings = c(""," ","#DIV/0!","NA"))

trainInds <- createDataPartition(raw$classe, p=.7, list = F)
#names, ids aren't likely predictors
#timestamps aren't in a good series, removing
train <- raw[trainInds, -c(1:5)] 
cv <- raw[-trainInds, -c(1:5)]

```  

# Exploratory steps:  

* Get a handle on numerics, discrete, continuous variables
* What can we remove?
** Ids, names and shit
** Too many NAs
** Near Zero Variance variables
* What else can we remove/create on the features?
** Manual correlations against outcome (scattermatrix) by groups
** Collinearity checks (VIF?)
** PCA on similar measures?

```{r eda}
# training set cleaning and analysis
# consider removing NZVs
train <- train[,-nearZeroVar(train, saveMetrics = F)]

# Impute NAs; NZV has also removed some NAs. I want to remove variables where
# more than 75% of the values are unknown.
train <- train[, -which(colSums(is.na(train))/nrow(train) > 0.75)]

# This pretty much got rid of all NAs. Looks like we don't need any imputing
# knnimpute of NAs?

# Remove low correlation items?
# ggpairs(data=train, columns=c(grep(".*(magnet_belt).*",colnames(train),value = F),54))
# Although we're not too concerned about multi-collinearity between predictors because we're going to be using classification trees, we still want to remove predictors that are very highly correlated to each other, by at least 95%
train.cor <- cor(train[,-54])
high.train.cor <- findCorrelation(train.cor, 0.95)
#visualize correlations
ggpairs(data=train, columns=high.train.cor,54)
train <- train[,-high.train.cor]

# not trying pca since we've removed some based on high correlations
```  

# BUILDING AND TUNING MODELS  

```{r train}
library(doMC)
# Might want to center and scale, want to try different algorithms
fit.bag <- train(classe~., data=train, preProcess=c('center','scale'), 
                   method="bagFDA", trControl = trainControl(
                           method="boot",
                           number = 5,
                           repeats = 3,
                           allowParallel = TRUE
                   ))

```  

# Training Models  

## Support Vector Machine w/ Gaussian Kernel 
## Random Forest

# Cross Validation  
## Confusion Matrix
## ROC


